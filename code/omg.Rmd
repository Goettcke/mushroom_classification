---
title: "Assignment 3 - Classification Study, DM855"
author: "Jonatan MÃ¸ller GÃ¸ttcke"
date: "2018 M05 3"
output:
  html_document: default
  pdf_document: default
---
setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) # This is for general case setting working directory.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## Setup of working
```{r working, include = TRUE} 
library(rstudioapi) 
library(e1071)
library(caret)
library(randomForest)
library(ggplot2)
library(knitr)
library(ROCR)
library(DiagrammeR)
library(caret)
library(mlbench)
library(pROC)
library(dplyr)
library(igraph)
library(ggraph)
library(class) # Order is important (masks knn function from igraph)
library(nnet)
``` 

#Reasoning for choosing the dataset 
The mushroom dataset, is an older dataset from 1987 and is quite well studied. It is classified as a classification dataset on the UCI website, because it contains labeled data, and has been used in many classification studies. It is interesting because it only contains nominal/categorical data, and my experience with nominal data is limited. In addition to being kind of a mycologist myself, and having a personal interest in solving this classifiation problem (to avoid being poisoned), I also expect decision trees to perform well in this domain, unlike some of the other models we have seen in the course such as Naïve Bayes. The dataset was also chosen because of the relatively high dimensionality and relatively high number of samples.      


## Loading data
```{r load_data, include=TRUE}
mushroom_data <- read.csv("../data/agaricus-lepiota.data", header = F,stringsAsFactors = TRUE)
names <- c("Class", "capshape","capsurface","capcolor","bruises","odor","gillattachment","gillspacing","gillsize","gillcolor","stalkshape","stalkroot","stalksurfaceabovering","stalksurfacebelowring","stalkcolorabovering","stalkcolorbelowring","veiltype","veilcolor",
"ringnumber","ringtype","sporeprintcolor","population","habitat")
length(names)
ncol(mushroom_data)


colnames(mushroom_data) = names #Renaming so we know what is class label, and which are attributes. Could also just have changed in the file, but in this way the reader can tag along. 
colnames(mushroom_data)

```


##Summary 
```{r summary, include=TRUE}
summary(mushroom_data)
set.seed(123) #Setting the random seed, for reproducibility 
``` 

So we have two classes edible and poisonous. We have 51.17 percent of the positive (edible class), so approximately equal class frequencies. 
Here we can see that column 17 (veiltype) only includes attributes with the value p, so this is a bit useless, and the randomForest algorithm, as used in can't handle this, so these are omitted. This attribute describes the veil-type which is either partial or universal. In mushrooms such as the *amanita caesarea* the veil is universal when the mushroom is young, and is broken as the mushroom grows. Ths is very common for adult mushrooms, and some like the famous *fly amanita* or as known by mycologist *amanita muscaria* has a very visible partial veil, which is the white dots on the red cap, that makes it so characteristic. It would be outside of the scope of this task to give more insight into the variables, but the reader can find more information here <https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.names>



```{r dropping_values, include=TRUE}
indices_without_17 <- c(1:16,18:ncol(mushroom_data))
 # The 17'th column only contains 1 attribute value, so it is omitted. 
mushroom_data <- mushroom_data[,indices_without_17]
mushroom_data_nolabel <- mushroom_data[,2:ncol(mushroom_data)]
summary(mushroom_data)
``` 

To have some test data and some training data, I do a simple split, so that we can build a model on a part of the data and validate on the other part of the data. 


##Gaining insight into the data
To gain insight into this relatively high dimensional data, many plots were made, these barplots, were very useful. I've attached all of them, but don't hesitate to skip them, if not interested. 
```{r barplots, include =TRUE}

for (i in 2:22) {
index <- i
names = c(rep("e",length(which(mushroom_data[,1] == "e"))),rep("p",length(which(mushroom_data[,1] == "p"))))
value = c(mushroom_data[which(mushroom_data[,1] =="e"),index],mushroom_data[which(mushroom_data[,1] =="p"),index])
data = data.frame(names,value)
attribute_boxplot <- qplot( x=names , y=value , data=data , geom=c("boxplot","jitter") , fill=names)
?qplot
print(attribute_boxplot + ggtitle(paste(names(mushroom_data[index]), "distribution in edible and poisonous mushrooms")))

}
```

###Attributes
Here the attributes are shown as boxplots, to see how the values for each attribute are distributed dependant on the class label. This way of plotting the data gives us a clear insight into how we can distinguish poisonous mushrooms from edible mushrooms, attribute by attribute and give an indication of what the classifiers will probably learn. The data is plotted in this way because it is nominal/categorical, and it doesn't make much sense to plot it in a cartesian coordinate system. Another reason for plotting the data in this manner, is because of the high dimensionality, so if we did 2-dimensional plots, there would be 22 choose 2 (231) combinations of plots to show. 
Some very similar attributes are *capcolor* and *capsurface* and very dissimilar attributes are for instance *odor* where some oder's in eadible mushrooms, are not at all present in poisonous mushrooms, for instance the anis smell (value 4), if this attribute is present, then we're sure, that the mushroom is edible. The different values for each attribute here depicted by numbers, are just numeric representations of the levels. For more insigt into the attribute levels, look at <https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.names> . 

##Defining useful functions
```{r useful_functions, include=TRUE} 
precision <- function(confucius_matrix) {confucius_matrix$table[1,1]/(confucius_matrix$table[1,1] + confucius_matrix$table[2,1])}
recall <- function(confucius_matrix){confucius_matrix$table[1,1]/(confucius_matrix$table[1,1] + confucius_matrix$table[1,2])}
f1_measure <- function(confucius_matrix) {2*precision(confucius_matrix)*recall(confucius_matrix)/(precision(confucius_matrix) + recall(confucius_matrix))}
accuracy <- function(confucius_matrix){(confucius_matrix$table[1,1]+confucius_matrix$table[2,2])/(sum(confucius_matrix$table))}
eval_summary <- function(confucius_matrix){
  print(paste("precision: ", precision(confucius_matrix)))
  print(paste("recall:", recall(confucius_matrix)))
  print(paste("f1_measure: ", f1_measure(confucius_matrix)))
  print(paste("accuracy: ", accuracy(confucius_matrix)))
}

```

##Splitting
```{r splitting, include=TRUE}
  split_index <- ceiling(nrow(mushroom_data) * 0.8)
  train_data <- mushroom_data[1:split_index,]
  test_data <- mushroom_data[(split_index +1):nrow(mushroom_data),]
```

Splitting the data into a training data-set and a test set, is done because all the tested algorithms, can work with this partitioning, the cross-validation implementations in R might not have support for all of the tested algorithms, but this is of course also an option. The split results in a roughly 2/3 majority of negative labled instances in the test data, but as this is dichotomous classification and the correct prediction of either class is seen as a correct classification, hence this imbalance in the test set is not seen as a noteworthy bias, although this also skews the balance of the training set (a bit).   

##Reasoning for choosing these classifiers. 
Naïve Bayes was chosen because it is a classifier, that uses probabilistic estimates to classify new query instances, based on the model build on the training data. This should work well with nominal data, because the distances between data points are probabilistic estimates and not *LP* distances. Knn was chosen because it is generally a good and simple classifier, and because the author was curious about the performance on nominal data, and how resources apply knn to nominal data. 
RandomForest in particular was chosen because it was assumed that decision trees in general would be well suited for this task, because of their ability to handle few categori attributes and large dimensional datasets. Random Forest is a highly applicable algorithm used as a go to standard in many research areas  (such as bioinformatics). 

## Naive Bayes Classifier
```{r NaïveBayes, include=TRUE}
model <- naiveBayes(Class ~ .,data = train_data)
nb_pred <- predict(model,test_data)


nb_avg_error <- mean(nb_pred != test_data[,1])
print(paste("avg_error",nb_avg_error)) # Just another evaluation measure

nb_confucius_matrix <- confusionMatrix(
                                      factor(nb_pred),
                                      factor(test_data[,1]))
nb_confucius_matrix # To output the result in comparison with the target.
print(nb_avg_error)
eval_summary(nb_confucius_matrix)

```


As can be seen here from the confusion matrix, there are only few true positives, when predicting if the mushrooms are edible. Which means we can't rely on our classification. The classification problem illustrates the necessity of making precise classifiers, cause with Naïve Bayes applied on this dataset we're only 72.98% certain that the prediction of an edible mushroom is not poisonous. The precision is calculated tp/(tp+fp), so the amount of true positives i.e. edible mushroms predicted as edible, out of total amount of mushrooms predicted as edible. 

##Knn classifier 
```{r Knn, include = TRUE} 
knn_train_data <- as.data.frame(sapply(train_data[,2:ncol(train_data)],as.integer)) # Changing the nominal data into integers, for knn to be able to understand it.
knn_test_data <- as.data.frame(sapply(test_data[,2:ncol(train_data)],as.integer))
target <- train_data[,1]

max_val = 0
best_K = -1

measure_list <- c()
for (i in 1:300) {
  knn_pred <- knn(knn_train_data,knn_test_data, target, k = i)
  knn_confucius_matrix <- confusionMatrix(factor(knn_pred),factor(test_data[,1]))
  #print(f1_measure(knn_confucius_matrix))
  measure_list <-c(measure_list, accuracy(knn_confucius_matrix) )
}

optimal_k <- which.is.max(measure_list)

paste("Optimal k:", optimal_k, " accuracy:", measure_list[optimal_k])
max(measure_list)


knn_pred <- knn(knn_train_data,knn_test_data, target, k = optimal_k)
#knn_pred # If we want to see the predictions

knn_confucius_matrix <- confusionMatrix(factor(knn_pred),factor(test_data[,1]))

knn_avg_error <- mean(knn_pred != test_data[,1])

knn_confucius_matrix 
print(knn_avg_error)
eval_summary(knn_confucius_matrix)

```

In the confusion matrix, it is clear that Knn, gives us a fairly decent classification, but for this specific case, we are less interested in accuracy and more interested in precision, so the 37 false positives resulting in a low nineties precision, is unacceptable. 
Choosing the right K in the K nearest neighbor classifier is a hard task. Duda et al. popularized, that a good rule of thumb is to take the square root of the number of training instances. An interval around this rule of thumb has been explored, but surprisingly found to be very limiting. Hence a larger interval, has been used to find a good k. This is based on the accuracy and hundreds of runs have been performed to find the best k. A perhaps more application specific evaluation would be to minimize the number of false positives / maximize the precision, because false positives in this specific case are potentially lethal. 


##Random Forest Classifier 
```{r random_forest, include = TRUE}
  rf_model <- randomForest(Class ~., data = train_data, importance=TRUE, proximity=TRUE, ntree=30)
  
accuracy_list <- c()  
  for (i in 1:50) {
    rf_model <- randomForest(Class ~., data = train_data, importance=TRUE, proximity=TRUE, ntree=i)
    rf_pred <- predict(rf_model, test_data)
    rf_confucius_matrix <- confusionMatrix(factor(rf_pred),test_data[,1])
    accuracy_list <- c(accuracy_list, accuracy(rf_confucius_matrix))
    if(accuracy(rf_confucius_matrix) == 1) {
      break; 
    }
  }

accuracy_list
barplot(accuracy_list,main="Accuracy change with number of trees")

optimal_nr_trees <- which.is.max(accuracy_list)

paste("Optimal nr_of_trees: ", optimal_nr_trees, " accuracy: ", accuracy_list[optimal_nr_trees])

#rf_model$predicted
rf_pred <- predict(rf_model, test_data)
rf_confucius_matrix <- confusionMatrix(factor(rf_pred),test_data[,1])
#rf_confucius_matrix
  
rf_avg_error <- mean(rf_pred != test_data[,1])
rf_avg_error # Just another evaluation measure

rf_confucius_matrix
eval_summary(rf_confucius_matrix)  
  
``` 



With a split at 80% training data and 20% test, the model performs impeccable i.e perfectly. The random forest only takes the number of trees as argument. The number of trees was incremented until a satisfying result was achieved, as can be seen in the barplot.  
<!--
If we split at 60% and test on 40 percent of the data, then we still obtain a decent classifier however, we have a fairly low sensitivity at 89.91% meaning if we take tp/(tp+fp), meaning out of the total amount of times our classifier recommends us to eat the mushroom, we don't get poisoned in 89% of the time. Now if we randomly pick 10 mushrooms, and use our classifier to predict if we should eat them, and let's say there's an even probability for the mushrooms being poisonous and edible, then we have a 34% chance of not being poisoned, so this is not the greatest classifier. -->

To see which attributes are important for the decision trees we can make a variable importance plot. 

##Variable importance plot 
```{r varimpplot, include=TRUE} 
varImpPlot(rf_model,main ="Variable Importance Plot")
```

In the variable importance plot, we can interpret the MeanDecreaseGini for a variable, as the variables ability to decrease the impurity. The mean is then how well this variable decrease the impurity on average over the splits where it's used. It is clear that *odor* is by far the most important variable, and then other attributes are less important, and by far according to MeanDecreaseGini. The importance of the variables, can be used to decrease the dimensionality, by only using the most important variables, this will decrease the variance of the model, and increase the bias. 


##Plotting the randomForest
```{r rf_plot, include = F}
library(dplyr)
library(ggraph)
library(igraph)
#This function is kindly taken from https://shiring.github.io/machine_learning/2017/03/16/rf_plot_ggraph
tree_func <- function(final_model, 
                      tree_num) {
  
  # get tree by index
  tree <- randomForest::getTree(final_model, 
                                k = tree_num, 
                                labelVar = TRUE) %>%
    tibble::rownames_to_column() %>%
    # make leaf split points to NA, so the 0s won't get plotted
    mutate(`split point` = ifelse(is.na(prediction), `split point`, NA))
  
  # prepare data frame for graph
  graph_frame <- data.frame(from = rep(tree$rowname, 2),
                            to = c(tree$`left daughter`, tree$`right daughter`))
  
  # convert to graph and delete the last node that we don't want to plot
  graph <- graph_from_data_frame(graph_frame) %>%
    delete_vertices("0")
  
  # set node labels
  V(graph)$node_label <- gsub("_", " ", as.character(tree$`split var`))
  V(graph)$leaf_label <- as.character(tree$prediction)
  V(graph)$split <- as.character(round(tree$`split point`, digits = 2))
  
  # plot
  plot <- ggraph(graph, 'dendrogram') + 
    theme_bw() +
    geom_edge_link() +
    geom_node_point() +
    geom_node_text(aes(label = node_label), na.rm = TRUE, repel = TRUE) +
    geom_node_label(aes(label = split), vjust = 2.5, na.rm = TRUE, fill = "white") +
    geom_node_label(aes(label = leaf_label, fill = leaf_label), na.rm = TRUE, 
					repel = TRUE, colour = "white", fontface = "bold", show.legend = FALSE) +
    theme(panel.grid.minor = element_blank(),
          panel.grid.major = element_blank(),
          panel.background = element_blank(),
          plot.background = element_rect(fill = "white"),
          panel.border = element_blank(),
          axis.line = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank(),
          axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          plot.title = element_text(size = 18))
  
  print(plot)
}

tree_num <- 1
```

```{r plotting_rf, include = TRUE}
tree_func(rf_model, tree_num)
```
The tree here gives a helpful interpretation to how a tree in the randomForrest behaves.  With the small number of trees used to achieve perfect classification it is possible to see exactly how the randomForrest makes decisions. 


##Confusion Matrices for model comparison 
The confusion matrices are put here, for comparison. 

![](confusion_nb.PNG) 
![](confusion_knn.PNG)
![](confusion_rf.PNG)

##Results 
In this section the results are summarized.  
```{r roc_curves,include=TRUE}
test_data_t <- as.numeric(test_data[,1] == "e")

nb_pred_t <- as.numeric(nb_pred == "e")
nb_pred_obj <- prediction(nb_pred_t,test_data_t)
nb_performance <- performance(nb_pred_obj,measure="tpr",x.measure="fpr")
plot(nb_performance,main = "ROC Curve for RandomForest on Mushroom data",col = "blue",lwd=3) 
abline(a = 0, b = 1, lwd = 2, lty = 2) #Inserting a diagonal line in the ROC plot 
nb_roc_auc <- performance(nb_pred_obj,measure = "auc")
nb_roc_auc@y.values # Gives the auc value


knn_pred_t <- as.numeric(knn_pred == "e")
knn_pred_obj <- prediction(knn_pred_t, test_data_t)
knn_performance <- performance(knn_pred_obj,measure = "tpr", x.measure = "fpr")

plot(knn_performance,main = "ROC Curve for K Nearest Neighbors on Mushroom data",col = "blue",lwd=3)
abline(a = 0, b = 1, lwd = 2, lty = 2)
knn_roc_auc <- performance(knn_pred_obj,measure = "auc")
knn_roc_auc@y.values # Gives the auc value


rf_pred_t <- as.numeric(rf_pred == "e")  #Done to make our data numeric so roc can understand the p and e names. 

#roc(test_data_t,pred_t,plot=TRUE) #I disliked this roc curve, because it was not visually precise enough. 

rf_pred_obj <- prediction(rf_pred_t,test_data_t)

rf_performance <- performance(rf_pred_obj,measure="tpr",x.measure="fpr")
plot(rf_performance,main = "ROC Curve for RandomForest on Mushroom data",col = "blue",lwd=3) # This plot is added because it gives a little bit more clarity.
abline(a = 0, b = 1, lwd = 2, lty = 2)
rf_roc_auc <- performance(rf_pred_obj,measure = "auc")
rf_roc_auc@y.values # Gives the auc value

```
In these ROC curves, it is clear that all of the classifiers are decent, and that knn and randomForest are best.

![](eval_summary.PNG)

In this study the importance of classifier precision is emphasized because misclassifying a poisonous mushroom can be lethal. Both Naïve Bayes and kNN have a to low precision, to be acceptable classifiers in this domain. Maybe this would be different if the classes were completely balanced (especially for Naïve Bayes), and k-fold cross-validation had been performed to train the models, instead of just the simple split.  


##Conclusion 
In this mini-study, the clear winner is the *randomForest*. There are many other methods, that could perform equivalently well, but the study was kept mini, as requested. Other methods to include could be Boosting trees like *XGBoost* and neural networks. It is interesting that Naïve Bayes performs so poorly in comparison with knn and randomForest. Future work could be to use a better translation of nominal data into numerical data that knn can understand, and to use cross-validation. In this mushroom dataset, it was shown, that odor is a very important feature and that tuning the classifiers paramters to perform well can be done, by trying large intervals for solutions. The results are very useful, and I could consider trying this on real data, that I gather myself, but some of the attributes are difficult to assess, like the most important attribute odor.   