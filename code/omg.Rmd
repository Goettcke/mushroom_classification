---
title: "moreFun"
author: "Jonatan Møller Gøttcke"
date: "2018 M05 3"
output: html_document
---
setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) # This is for general case setting working directory.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## Setting working
```{r working, include = TRUE} 
library(rstudioapi) 
library(e1071)
library(caret)
library(randomForest)
library(ggplot2)
library(knitr)
library(ROCR)
library(DiagrammeR)
library(caret)
library(mlbench)
library(pROC)
#TODO Show tree growth, and argue, that this is how you set your parameters, in the 
#TODO Argue about the class distribution on the data that we're trying to predict, as shown below  
#x <- summary(test_data)
#x[1:2]
#TODO pretty plot confusion matrices
#TODO summarize the avg. error in a nice bar plot
#TODO represent the data in a customized way, so that it is possible to see the data visually. Maybe make barplots, that show how many of the samples have the different categories for each variable. So like in odor 2000 stinky 3000 føj for helvede. and so on. 
#Compare the AUC for all the algorithms. 
#TODO Write conclusion which  sums the insights, and shows future work, that could be done.
#Write about the selection of methods, and why these precisely were chosen.
``` 

#Reasoning for choosing the dataset 
The mushroom dataset, is an older dataset from 1987 and is quite well studied. It is classified as a classification dataset on the UCI website, because it contains labled data, and has been used in many classification studies. It is interesting because it only contains nominal/categorical data, and my experience with nominal data is limited. In addition to being kind of a mycologist myself, and having a personal interest in solving this classifiation problem (to avoid being poisoned), I also expect decision trees to perform well in this domain, unlike some of the other models we have seen in the course such as Naïve Bayes. The dataset was also chosen because of the relatively high dimensionality and relatively high number of samples.      


## Loading data
```{r load_data, include=TRUE}
mushroom_data <- read.csv("../data/agaricus-lepiota.data", header = F,stringsAsFactors = TRUE)
names <- c("Class", "capshape","capsurface","capcolor","bruises","odor","gillattachment","gillspacing","gillsize","gillcolor","stalkshape","stalkroot","stalksurfaceabovering","stalksurfacebelowring","stalkcolorabovering","stalkcolorbelowring","veiltype","veilcolor",
"ringnumber","ringtype","sporeprintcolor","population","habitat")
length(names)
ncol(mushroom_data)


colnames(mushroom_data) = names #Renaming so we know this is the Class, and what the attributes are. Could also just have changed in the file, but in this way the reader can tag along. 
colnames(mushroom_data)

```


##Summary 
```{r summary, include=TRUE}
summary(mushroom_data)
set.seed(123) #Setting the random seed, for reproducibility 
``` 

So we have two classes edible and poisonous. We have 51.17 percent of the positive (edible class), so approximately equal class frequencies. 
Here we can see that column 17 only includes attributes with the value p, so this is a bit useless, and the randomForest algorithm "rf", as used in crossvalidation can handle this, so these are omitted. This attribute describes the veil-type which is a either partial or universal. In mushrooms such as the *amanita caesarea* the veil is universal when the musfhroom is young, and is broken as the mushroom grows. Ths is very common for adult mushrooms, and some like the famous *fly amanita* or as known by mycologist *amanita muscaria* has a very visible partial veil, which is the white dots on the red cap, that makes it so characteristic. It would be outside of the scope of this task to give more insight into the variables, but the reader can find more information here <https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.names>



```{r dropping_values, include=TRUE}
indices_without_17 <- c(1:16,18:ncol(mushroom_data))
 # The 17'th column only contains 1 attribute value, so it is omitted, because is useless information. 
mushroom_data <- mushroom_data[,indices_without_17]
mushroom_data_nolabel <- mushroom_data[,2:ncol(mushroom_data)]
summary(mushroom_data)
``` 

To have some test data and some training data, I do a simple split, so that we can build a model on a part of the data and validate on the other part of the data. 

##Splitting
```{r splitting, include=TRUE}
  split_index <- ceiling(nrow(mushroom_data) * 0.8)
  train_data <- mushroom_data[1:split_index,]
  test_data <- mushroom_data[(split_index +1):nrow(mushroom_data),]
```


## Naive Bayes Classifier
```{r NaïveBayes, include=TRUE}
model <- naiveBayes(Class ~ .,data = train_data)
nb_pred <- predict(model,test_data)
correct_predictions <- nb_pred == test_data[,1]
#correct_predictions
t_correct_predictions <- as.data.frame(correct_predictions) #This is to get an overview of which are correctly classified and which are incorrectly classified. 
length(which(correct_predictions == TRUE %in% t_correct_predictions)) #Counting the number of values that are classified correctly. 


nb_avg_error <- mean(nb_pred != test_data[,1])
nb_avg_error # Just another evaluation measure

nb_confucius_matrix <- confusionMatrix(
                                      factor(nb_pred),
                                      factor(test_data[,1]))
nb_confucius_matrix # To output the result in comparison with the target. 
```


As can be seen here from the confusion matrix, there are only few true positives, when predicting if the mushrooms are edible. Which means we can't rely on our classification. The classification problem illustrates the necessity of making sensitive classifiers, cause here we're only 72.98% certain that the prediction of an edible mushroom is not poisonous. Here the sensitivity is calculated tp/(tp+fp), so the amount of true positives i.e. edible mushroms predicted as edible, out of total amount of mushrooms predicted as edible.  This is not good enough.! Hence we continue the search for a better classifier.   



##Random Forest Classifier 
```{r random_forest, include = TRUE}
  rf_model <- randomForest(Class ~., data = train_data, importance=TRUE, proximity=TRUE, ntree=10)

#rf_model$predicted
  rf_pred <- predict(rf_model, test_data)
  rf_confucius_matrix <- confusionMatrix(factor(rf_pred),test_data[,1])
  rf_confucius_matrix
  avg_error <- mean(rf_pred != test_data[,1])
  avg_error # Just another evaluation measure
``` 



Here we can see, that with the split at 90%, the model still performs impeccable. If we split at 60% and test on 40 percent of the data, then we still obtain a decent classifier however, we have a fairly low sensitivity at 89.91% meaning if we take tp/(tp+fp), meaning out of the total amount of times our classifier recommends us to eat the mushroom, we don't get poisoned in 89% of the time. Now if we randomly pick 10 mushrooms, and use our classifier to predict if we should eat them, and let's say there's an even probability for the mushrooms being poisonous and edible, then we have a 34% chance of not being poisoned, so this is not the greatest classifier. 

To see which attributes are important for the decision tree's we can make a variable importance plot. 

##Variable importance plot 
```{r varimpplot, include=TRUE} 
varImpPlot(rf_model,main ="Variable Importance Plot")
```

In the variable importance plot, we can interpret the MeanDecreaseGini for a variable, as the variables ability to decrease the impurity. The mean is then how well this variable decrease the impurity on average over it splits. It is clear that *odor* is by far the most important variable, and then *gillcolor* and *stalksurfaceabovering*. The importance of the variables, can be used to decrease the dimensionality, by only using the most important variables, this will decrease the variance of the model, and then be a part of avoiding overfitting the model, or as a tool to decrease computational complexity. 


##Roc curves for model comparison
Now we have seen some classifiers, let's plot some ROC curves. 
```{r roc_curves,include=TRUE}
test_data_t <- as.numeric(test_data[,1] == "e")


nb_pred_t <- as.numeric(nb_pred == "e")
nb_pred_obj <- prediction(nb_pred_t,test_data_t)
nb_performance <- performance(nb_pred_obj,measure="tpr",x.measure="fpr")
plot(nb_performance,main = "ROC Curve for RandomForest on Mushroom data",col = "blue",lwd=3) 
abline(a = 0, b = 1, lwd = 2, lty = 2) #Inserting a diagonal line in the ROC plot 


rf_pred_t <- as.numeric(rf_pred == "e")  #Done to make our data numeric so roc can understand the p and e names. 

#roc(test_data_t,pred_t,plot=TRUE) #I disliked this roc curve, because it was not visually precise enough. 

rf_pred_obj <- prediction(rf_pred_t,test_data_t)

rf_performance <- performance(rf_pred_obj,measure="tpr",x.measure="fpr")
plot(rf_performance,main = "ROC Curve for RandomForest on Mushroom data",col = "blue",lwd=3) # This plot is added because it gives a little bit more clarity.
abline(a = 0, b = 1, lwd = 2, lty = 2)
```